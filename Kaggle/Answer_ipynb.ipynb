{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Answer.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8EPmcSKihJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INITIAL PACKAGES\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from utils import load_data, run"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wyLgZcojf8b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "b911a655-b403-42c2-d0dc-43308cbe444e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJJF0rLsjgYR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "249c9d78-8371-494d-8c91-3db047550eda"
      },
      "source": [
        "gdrive_root = '/content/gdrive/My Drive'\n",
        "data_path = os.path.join(gdrive_root, 'final_project')\n",
        "os.listdir(data_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sample_submission.csv',\n",
              " 'test_id.csv',\n",
              " 'test_images.npy',\n",
              " 'train_id_label.csv',\n",
              " 'train_images.npy',\n",
              " 'valid_images.npy',\n",
              " 'valid_id_label.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzhyudX2lZA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data, test_data = load_data(data_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOwiJzbhlriu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "12256257-7101-457c-d26d-181ea96277f4"
      },
      "source": [
        "print(f'train id label:\\n {train_data[0].head()}')\n",
        "print(f'train images shape: {train_data[1].shape}\\n')\n",
        "assert len(train_data[0]) == len(train_data[1])\n",
        "\n",
        "print(f'valid id label:\\n {valid_data[0].head()}')\n",
        "print(f'valid images shape: {valid_data[1].shape}\\n')\n",
        "assert len(valid_data[0]) == len(valid_data[1])\n",
        "\n",
        "print(f'test id:\\n {test_data[0].head()}')\n",
        "print(f'test images shape: {test_data[1].shape}\\n')\n",
        "assert len(test_data[0]) == len(test_data[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train id label:\n",
            "            id  label\n",
            "0  a1cec2874d      1\n",
            "1  ddbe361041      7\n",
            "2  910628fd4e      2\n",
            "3  171ae22c4b     10\n",
            "4  f1e68a9c42      1\n",
            "train images shape: (100000, 3, 32, 32)\n",
            "\n",
            "valid id label:\n",
            "            id  label\n",
            "0  f0829f4147      7\n",
            "1  91116a7846     11\n",
            "2  88c83f1240      3\n",
            "3  a7cd83fe4f      4\n",
            "4  78b3ce0c46      1\n",
            "valid images shape: (10000, 3, 32, 32)\n",
            "\n",
            "test id:\n",
            "            id\n",
            "0  edf4ea9a4b\n",
            "1  7496e3e847\n",
            "2  e0f2110942\n",
            "3  9fb87df04a\n",
            "4  0a3608ca47\n",
            "test images shape: (10000, 3, 32, 32)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJrqhMr_luum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORT PACKAGES YOU NEED\n",
        "from time import time\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import math\n",
        "\n",
        "import torch.nn.parallel\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=14):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])\n",
        "def DMI_loss(output, target):\n",
        "    outputs = F.softmax(output, dim=1)\n",
        "    targets = target.reshape(target.size(0), 1).cpu()\n",
        "    y_onehot = torch.FloatTensor(target.size(0), 14).zero_()\n",
        "    y_onehot.scatter_(1, targets, 1)\n",
        "    y_onehot = y_onehot.transpose(0, 1).cuda()\n",
        "    mat = y_onehot @ outputs\n",
        "    mat = mat / target.size(0)\n",
        "    return -1.0 * torch.log(torch.abs(torch.det(mat.float())) + 0.001)     "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-rnuDltx60A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nlMH6hrl1Vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_predict(train_data, valid_data, test_data):\n",
        "    \"\"\"Train a model and return prediction on test images.\n",
        "\n",
        "    Given train and valid data, build your model and optimize.\n",
        "    Then, return predictions on test_images.\n",
        "\n",
        "    You can import packages you want inside 'EDIT HERE' as long as they are permitted.\n",
        "    (See document for the list of possible packages)\n",
        "\n",
        "    arguments:\n",
        "        train_data: tuple of (pandas.DataFrame, np.array).\n",
        "        - 0: pandas.DataFrame with columns ['id', 'label']\n",
        "          'id' contains unique id assigned to each image.\n",
        "          'label' contains label (0 ~ # classes-1) corresponding to its image.\n",
        "        - 1: train image in np.array of (# train data, # channel, height, width)\n",
        "\n",
        "        valid_data: tuple of (pandas.DataFrame, np.array).\n",
        "        - 0: pandas.DataFrame with columns ['id', 'label']\n",
        "          'id' contains unique id assigned to each image.\n",
        "          'label' contains label (0 ~ # classes-1) corresponding to its image.\n",
        "        - 1: valid image in np.array of (# valid data, # channel, height, width)\n",
        "\n",
        "        test_data: tuple of (pandas.DataFrame, np.array).\n",
        "        - 0: pandas.DataFrame with columns ['id']\n",
        "          'id' contains unique id assigned to each image.\n",
        "        - 1: test image in np.array of (# test data, # channel, height, width)\n",
        "    \n",
        "    returns:\n",
        "        pandas.DataFrame, predictions on test images with columns ['id', 'label'].\n",
        "        'id' should contain unique id assigned to test images. \n",
        "        'label' should contain prediction on the test image correspond to its id\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    LEARNING_RATE = 0.00001\n",
        "    BATCH_SIZE = 128\n",
        "    TEST_BATCH_SIZE = 1024\n",
        "    NUM_EPOCHS = 150\n",
        "    PATIENCE = 10\n",
        "    ENDURE = 0\n",
        "\n",
        "    train_id_label, train_images = train_data\n",
        "    valid_id_label, valid_images = valid_data\n",
        "    test_id, test_images = test_data\n",
        "\n",
        "    num_train = len(train_images)\n",
        "    num_valid = len(valid_images)\n",
        "    num_test = len(test_images)\n",
        "    \n",
        "    # Convert data into torch.Tensor\n",
        "    x_train = torch.FloatTensor(train_images)\n",
        "    y_train = train_id_label['label'].values\n",
        "    y_train = torch.LongTensor(y_train)\n",
        "\n",
        "    x_valid = torch.FloatTensor(valid_images)\n",
        "    y_valid = valid_id_label['label'].values\n",
        "    y_valid = torch.LongTensor(y_valid)\n",
        "\n",
        "    x_test = torch.FloatTensor(test_images)\n",
        "\n",
        "    num_features = np.prod(x_train.shape[1:])\n",
        "    num_classes = int(y_train.max()) + 1\n",
        "\n",
        "    # Build torch dataset, dataloader\n",
        "    train_dataset = TensorDataset(x_train, y_train)\n",
        "    valid_dataset = TensorDataset(x_valid, y_valid)\n",
        "    test_dataset = TensorDataset(x_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Build CNN model\n",
        "    model = ResNet50().cuda()\n",
        "    #odel.load_state_dict(torch.load('best_mlp_real.p'))\n",
        "\n",
        "\n",
        "    # Optimizer and loss function\n",
        "    optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, weight_decay=1e-4, lr=LEARNING_RATE)\n",
        "    \n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train model\n",
        "    mean_train_losses = []\n",
        "    mean_valid_losses = []\n",
        "    valid_acc_list = []\n",
        "    best_acc = -1\n",
        "\n",
        "    train_s = time()\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        epoch_s = time()\n",
        "        model.train()\n",
        "        \n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            #images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            images = torch.autograd.Variable(images.cuda())\n",
        "            labels = torch.autograd.Variable(labels.cuda())\n",
        "          \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = loss_fn(outputs,labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_losses.append(loss.item())\n",
        "            \n",
        "        model.eval()\n",
        "        \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        # Disable gradient calculation for memory, computation efficiency\n",
        "        with torch.no_grad():\n",
        "            for i, (images, labels) in enumerate(valid_loader):\n",
        "                #images, labels = images.to(device), labels.to(device)\n",
        "                images = torch.autograd.Variable(images.cuda())\n",
        "                labels = torch.autograd.Variable(labels.cuda())\n",
        "            \n",
        "                outputs = model(images)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                \n",
        "                valid_losses.append(loss.item())\n",
        "                \n",
        "                predicted = torch.argmax(outputs.data, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "                \n",
        "        mean_train_losses.append(np.mean(train_losses))\n",
        "        mean_valid_losses.append(np.mean(valid_losses))\n",
        "\n",
        "        epoch_elapsed = time() - epoch_s\n",
        "        \n",
        "        valid_acc = correct / total\n",
        "        valid_acc_list.append(valid_acc)\n",
        "        print('epoch: {}, train loss: {:.4f}, valid loss: {:.4f}, valid acc: {:.4f}, elapsed: {:.4f}'\\\n",
        "            .format(epoch, np.mean(train_losses), np.mean(valid_losses), valid_acc, epoch_elapsed))\n",
        "        \n",
        "        if best_acc < valid_acc:\n",
        "            print('Best Accuracy updated (%.4f => %.4f)' % (best_acc, valid_acc))\n",
        "            best_acc = valid_acc\n",
        "            best_epoch = epoch\n",
        "            ENDURE = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), 'best_mlp.p')\n",
        "        else:\n",
        "            ENDURE += 1\n",
        "            if ENDURE >= PATIENCE:\n",
        "                print('Early stop triggered...!')\n",
        "                break\n",
        "    train_elapsed = time() - train_s\n",
        "    print('Training Finished...!!')\n",
        "    print('Time: %.4f' % train_elapsed)\n",
        "    print('Best Valid acc : %.4f at epoch %d' % (best_acc, best_epoch))\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_mlp.p'))\n",
        "    model.eval()\n",
        "\n",
        "    # Make prediction on test data\n",
        "    test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for i, (images, ) in enumerate(test_loader):\n",
        "            #images = images.to(device)\n",
        "            images = torch.autograd.Variable(images.cuda())\n",
        "        \n",
        "            outputs = model(images)\n",
        "            \n",
        "            pred = outputs.argmax(1)\n",
        "\n",
        "            if device == 'cuda':\n",
        "                test_preds += pred.detach().cpu().numpy().tolist()\n",
        "            else:\n",
        "                test_preds += pred.detach().numpy().tolist()\n",
        "    \n",
        "    # Return prediction\n",
        "    test_id['label'] = test_preds\n",
        "    pred = test_id.loc[:, ['id', 'label']]\n",
        "\n",
        "    return pred"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeTK0Q_Fm0Pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run(train_and_predict, train_data, valid_data, test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRxtn3epoNnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_predict_again(train_data, valid_data, test_data):\n",
        "    \"\"\"Train a model and return prediction on test images.\n",
        "\n",
        "    Given train and valid data, build your model and optimize.\n",
        "    Then, return predictions on test_images.\n",
        "\n",
        "    You can import packages you want inside 'EDIT HERE' as long as they are permitted.\n",
        "    (See document for the list of possible packages)\n",
        "\n",
        "    arguments:\n",
        "        train_data: tuple of (pandas.DataFrame, np.array).\n",
        "        - 0: pandas.DataFrame with columns ['id', 'label']\n",
        "          'id' contains unique id assigned to each image.\n",
        "          'label' contains label (0 ~ # classes-1) corresponding to its image.\n",
        "        - 1: train image in np.array of (# train data, # channel, height, width)\n",
        "\n",
        "        valid_data: tuple of (pandas.DataFrame, np.array).\n",
        "        - 0: pandas.DataFrame with columns ['id', 'label']\n",
        "          'id' contains unique id assigned to each image.\n",
        "          'label' contains label (0 ~ # classes-1) corresponding to its image.\n",
        "        - 1: valid image in np.array of (# valid data, # channel, height, width)\n",
        "\n",
        "        test_data: tuple of (pandas.DataFrame, np.array).\n",
        "        - 0: pandas.DataFrame with columns ['id']\n",
        "          'id' contains unique id assigned to each image.\n",
        "        - 1: test image in np.array of (# test data, # channel, height, width)\n",
        "    \n",
        "    returns:\n",
        "        pandas.DataFrame, predictions on test images with columns ['id', 'label'].\n",
        "        'id' should contain unique id assigned to test images. \n",
        "        'label' should contain prediction on the test image correspond to its id\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    LEARNING_RATE = 0.0001\n",
        "    BATCH_SIZE = 64\n",
        "    TEST_BATCH_SIZE = 1024\n",
        "    NUM_EPOCHS = 150\n",
        "    PATIENCE = 10\n",
        "    ENDURE = 0\n",
        "\n",
        "    train_id_label, train_images = train_data\n",
        "    valid_id_label, valid_images = valid_data\n",
        "    test_id, test_images = test_data\n",
        "\n",
        "    num_train = len(train_images)\n",
        "    num_valid = len(valid_images)\n",
        "    num_test = len(test_images)\n",
        "    \n",
        "    # Convert data into torch.Tensor\n",
        "    x_train = torch.FloatTensor(train_images)\n",
        "    y_train = train_id_label['label'].values\n",
        "    y_train = torch.LongTensor(y_train)\n",
        "\n",
        "    x_valid = torch.FloatTensor(valid_images)\n",
        "    y_valid = valid_id_label['label'].values\n",
        "    y_valid = torch.LongTensor(y_valid)\n",
        "\n",
        "    x_test = torch.FloatTensor(test_images)\n",
        "\n",
        "    num_features = np.prod(x_train.shape[1:])\n",
        "    num_classes = int(y_train.max()) + 1\n",
        "\n",
        "    # Build torch dataset, dataloader\n",
        "    train_dataset = TensorDataset(x_train, y_train)\n",
        "    valid_dataset = TensorDataset(x_valid, y_valid)\n",
        "    test_dataset = TensorDataset(x_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Build CNN model\n",
        "    model = ResNet50().cuda()\n",
        "    model.load_state_dict(torch.load('best_mlp.p'))\n",
        "\n",
        "\n",
        "    # Optimizer and loss function\n",
        "    optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, weight_decay=1e-4, lr=LEARNING_RATE)\n",
        "    \n",
        "    loss_fn = DMI_loss\n",
        "\n",
        "    # Train model\n",
        "    mean_train_losses = []\n",
        "    mean_valid_losses = []\n",
        "    valid_acc_list = []\n",
        "    best_acc = -1\n",
        "\n",
        "    train_s = time()\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        epoch_s = time()\n",
        "        model.train()\n",
        "\n",
        "        if(epoch >= 5) :\n",
        "          LEARNING_RATE = 0.00005\n",
        "          optimizer.lr = 0.00005\n",
        "        \n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            #images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            images = torch.autograd.Variable(images.cuda())\n",
        "            labels = torch.autograd.Variable(labels.cuda())\n",
        "          \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = loss_fn(outputs,labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_losses.append(loss.item())\n",
        "            \n",
        "        model.eval()\n",
        "        \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        # Disable gradient calculation for memory, computation efficiency\n",
        "        with torch.no_grad():\n",
        "            for i, (images, labels) in enumerate(valid_loader):\n",
        "                #images, labels = images.to(device), labels.to(device)\n",
        "                images = torch.autograd.Variable(images.cuda())\n",
        "                labels = torch.autograd.Variable(labels.cuda())\n",
        "            \n",
        "                outputs = model(images)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                \n",
        "                valid_losses.append(loss.item())\n",
        "                \n",
        "                predicted = torch.argmax(outputs.data, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "                \n",
        "        mean_train_losses.append(np.mean(train_losses))\n",
        "        mean_valid_losses.append(np.mean(valid_losses))\n",
        "\n",
        "        epoch_elapsed = time() - epoch_s\n",
        "        \n",
        "        valid_acc = correct / total\n",
        "        valid_acc_list.append(valid_acc)\n",
        "        print('epoch: {}, train loss: {:.4f}, valid loss: {:.4f}, valid acc: {:.4f}, elapsed: {:.4f}'\\\n",
        "            .format(epoch, np.mean(train_losses), np.mean(valid_losses), valid_acc, epoch_elapsed))\n",
        "        \n",
        "        if best_acc < valid_acc:\n",
        "            print('Best Accuracy updated (%.4f => %.4f)' % (best_acc, valid_acc))\n",
        "            best_acc = valid_acc\n",
        "            best_epoch = epoch\n",
        "            ENDURE = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), 'best_mlp_real.p')\n",
        "        else:\n",
        "            ENDURE += 1\n",
        "            if ENDURE >= PATIENCE:\n",
        "                print('Early stop triggered...!')\n",
        "                break\n",
        "    train_elapsed = time() - train_s\n",
        "    print('Training Finished...!!')\n",
        "    print('Time: %.4f' % train_elapsed)\n",
        "    print('Best Valid acc : %.4f at epoch %d' % (best_acc, best_epoch))\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_mlp_real.p'))\n",
        "    model.eval()\n",
        "\n",
        "    # Make prediction on test data\n",
        "    test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for i, (images, ) in enumerate(test_loader):\n",
        "            #images = images.to(device)\n",
        "            images = torch.autograd.Variable(images.cuda())\n",
        "        \n",
        "            outputs = model(images)\n",
        "            \n",
        "            pred = outputs.argmax(1)\n",
        "\n",
        "            if device == 'cuda':\n",
        "                test_preds += pred.detach().cpu().numpy().tolist()\n",
        "            else:\n",
        "                test_preds += pred.detach().numpy().tolist()\n",
        "    \n",
        "    # Return prediction\n",
        "    test_id['label'] = test_preds\n",
        "    pred = test_id.loc[:, ['id', 'label']]\n",
        "\n",
        "    return pred"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpFCym31pTxm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "9ae1d03e-fda5-438d-9720-09a7550d62ae"
      },
      "source": [
        "run(train_and_predict_again, train_data, valid_data, test_data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, train loss: 0.4247, valid loss: 2.0544, valid acc: 0.4968, elapsed: 720.5079\n",
            "Best Accuracy updated (-1.0000 => 0.4968)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6f47d373bb80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_and_predict_again\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(train_and_predict, train_data, valid_data, test_images)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_and_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0msave_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-bcc575a1d262>\u001b[0m in \u001b[0;36mtrain_and_predict_again\u001b[0;34m(train_data, valid_data, test_data)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}